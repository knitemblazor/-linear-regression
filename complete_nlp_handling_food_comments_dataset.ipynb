{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "complete nlp handling food comments dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "scrolled": true,
        "id": "8IQKqdE3Ob78",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import print_function\n",
        "import collections\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from six.moves import urllib\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import lxml.html\n",
        "import io\n",
        "from lxml.cssselect import CSSSelector\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "\n",
        "YOUTUBE_COMMENTS_URL = 'https://www.youtube.com/all_comments?v={youtube_id}'\n",
        "YOUTUBE_COMMENTS_AJAX_URL = 'https://www.youtube.com/comment_ajax'\n",
        "\n",
        "USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'\n",
        "\n",
        "comments=0\n",
        "def find_value(html, key, num_chars=2):\n",
        "    pos_begin = html.find(key) + len(key) + num_chars\n",
        "    pos_end = html.find('\"', pos_begin)\n",
        "    return html[pos_begin: pos_end]\n",
        "\n",
        "\n",
        "def extract_comments(html):\n",
        "    tree = lxml.html.fromstring(html)\n",
        "    item_sel = CSSSelector('.comment-item')\n",
        "    text_sel = CSSSelector('.comment-text-content')\n",
        "    time_sel = CSSSelector('.time')\n",
        "    author_sel = CSSSelector('.user-name')\n",
        "\n",
        "    for item in item_sel(tree):\n",
        "        yield {'cid': item.get('data-cid'),\n",
        "               'text': text_sel(item)[0].text_content(),\n",
        "               'time': time_sel(item)[0].text_content().strip(),\n",
        "               'author': author_sel(item)[0].text_content()}\n",
        "\n",
        "\n",
        "def extract_reply_cids(html):\n",
        "    tree = lxml.html.fromstring(html)\n",
        "    sel = CSSSelector('.comment-replies-header > .load-comments')\n",
        "    return [i.get('data-cid') for i in sel(tree)]\n",
        "\n",
        "\n",
        "def ajax_request(session, url, params, data, retries=10, sleep=20):\n",
        "    for _ in range(retries):\n",
        "        response = session.post(url, params=params, data=data)\n",
        "        if response.status_code == 200:\n",
        "            response_dict = json.loads(response.text)\n",
        "            return response_dict.get('page_token', None), response_dict['html_content']\n",
        "        else:\n",
        "            time.sleep(sleep)\n",
        "\n",
        "\n",
        "def download_comments(youtube_id, sleep=1):\n",
        "    session = requests.Session()\n",
        "    session.headers['User-Agent'] = USER_AGENT\n",
        "\n",
        "    # Get Youtube page with initial comments\n",
        "    response = session.get(YOUTUBE_COMMENTS_URL.format(youtube_id=youtube_id))\n",
        "    html = response.text\n",
        "    reply_cids = extract_reply_cids(html)\n",
        "\n",
        "    ret_cids = []\n",
        "    for comment in extract_comments(html):\n",
        "        ret_cids.append(comment['cid'])\n",
        "        yield comment\n",
        "\n",
        "    page_token = find_value(html, 'data-token')\n",
        "    session_token = find_value(html, 'XSRF_TOKEN', 4)\n",
        "\n",
        "    first_iteration = True\n",
        "\n",
        "    # Get remaining comments (the same as pressing the 'Show more' button)\n",
        "    while page_token:\n",
        "        data = {'video_id': youtube_id,\n",
        "                'session_token': session_token}\n",
        "\n",
        "        params = {'action_load_comments': 1,\n",
        "                  'order_by_time': True,\n",
        "                  'filter': youtube_id}\n",
        "\n",
        "        if first_iteration:\n",
        "            params['order_menu'] = True\n",
        "        else:\n",
        "            data['page_token'] = page_token\n",
        "\n",
        "        response = ajax_request(session, YOUTUBE_COMMENTS_AJAX_URL, params, data)\n",
        "        if not response:\n",
        "            break\n",
        "\n",
        "        page_token, html = response\n",
        "\n",
        "        reply_cids += extract_reply_cids(html)\n",
        "        for comment in extract_comments(html):\n",
        "            if comment['cid'] not in ret_cids:\n",
        "                ret_cids.append(comment['cid'])\n",
        "                yield comment\n",
        "\n",
        "        first_iteration = False\n",
        "        time.sleep(sleep)\n",
        "\n",
        "    # Get replies (the same as pressing the 'View all X replies' link)\n",
        "    for cid in reply_cids:\n",
        "        data = {'comment_id': cid,\n",
        "                'video_id': youtube_id,\n",
        "                'can_reply': 1,\n",
        "                'session_token': session_token}\n",
        "\n",
        "        params = {'action_load_replies': 1,\n",
        "                  'order_by_time': True,\n",
        "                  'filter': youtube_id,\n",
        "                  'tab': 'inbox'}\n",
        "\n",
        "        response = ajax_request(session, YOUTUBE_COMMENTS_AJAX_URL, params, data)\n",
        "        if not response:\n",
        "            break\n",
        "\n",
        "        _, html = response\n",
        "\n",
        "        for comment in extract_comments(html):\n",
        "            if comment['cid'] not in ret_cids:\n",
        "                ret_cids.append(comment['cid'])\n",
        "                yield comment\n",
        "        time.sleep(sleep)\n",
        "\n",
        "def main():\n",
        "  youtube_id = \"zH5btDbjLkc\"\n",
        "  limit = 6300\n",
        "\n",
        "  a=[]\n",
        "  for i in range(0,limit):\n",
        "    a.append(0)\n",
        "  print('Downloading Youtube comments for video:', youtube_id)\n",
        "  count=0\n",
        "\n",
        "  for comment in download_comments(youtube_id):\n",
        "    a[count]=(json.dumps(comment, ensure_ascii=True))\n",
        "    a[count]=yaml.load(a[count])\n",
        "    a[count]=a[count]\n",
        "    b=a[count]\n",
        "    for k,v in b.items():\n",
        "      b[k] = \"\".join(map(lambda c: c if 32<ord(c)<127 else \" \" , v)) #removing non ascii char from a dictionary\n",
        "    a[count]=b\n",
        "    count += 1\n",
        "    sys.stdout.write('Downloaded %d comment(s)\\r' % count)\n",
        "    sys.stdout.flush()\n",
        "    if limit and count >= limit:\n",
        "      break\n",
        "  print('\\nDone!')\n",
        "\n",
        "\n",
        "  j=[]\n",
        "  for i in range (0,limit):\n",
        "    j.append(pd.DataFrame([a[i]]))\n",
        "  df=pd.concat(j)\n",
        "\n",
        "\n",
        "  columns_for_differencing = ['cid','time','author','Index']\n",
        "  df= df.copy()[df.columns.difference(columns_for_differencing)]\n",
        "  df = df.astype(str)\n",
        "  b=np.array(df[1:limit])\n",
        "\n",
        "  d=[]\n",
        "  for i in range(0,len(b)):\n",
        "    d.append(0)\n",
        "\n",
        "  for j in range(0,len(b)):\n",
        "    for i in b[j]:\n",
        "        d[j]=str(i)\n",
        "  with open('comments.txt', 'a') as f:\n",
        "    for item in d:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "  words1=[]\n",
        "  words =[]\n",
        "  stopwords=[]\n",
        "  os.chdir(\"/home/her/tourist review/latest\")\n",
        "  filename = 'stopwords.txt'\n",
        "  file = open(filename,\"r\" )\n",
        "  lines = file.read().splitlines()\n",
        "  stopwords=lines\n",
        "  print(stopwords)\n",
        "      \n",
        "\n",
        "  for i in range(0,len(d)):\n",
        "    words1=d[i].split()\n",
        "    for j in range(0,len(words1)):\n",
        "        words.append(words1[j])\n",
        "  wordsf=[]\n",
        "  words = [word.lower() for word in words]\n",
        "  words = [word for word in words if word.isalpha()]\n",
        "  wordsf=(set(words).difference(stopwords))           \n",
        "  print(wordsf)\n",
        "  \n",
        "#   return(words) \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tmain()\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r3NJzhqdc0ZM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LABELLING SECTION"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "jRwsUtdnOb8f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/home/her/tourist review/latest\")\n",
        "\n",
        "file = open('comments.txt',\"r\") #comments of  delhi video tours  especially food related\n",
        "d= file.read().splitlines()  \n",
        "print(len(d),\"of comments in the given dataset\")\n",
        "file.close()\n",
        "\n",
        " \n",
        "\n",
        "filename = 'negative.txt'\n",
        "file = open(filename,\"r\" )\n",
        "negative = file.read().splitlines()\n",
        "file.close()   \n",
        "\n",
        "filename = 'positive.txt'\n",
        "file = open(filename,\"r\" )\n",
        "positive = file.read().splitlines()\n",
        "file.close()\n",
        "\n",
        "lab=[]\n",
        "for i in range(len(d)):\n",
        "    lab.append(0)\n",
        "def label(d,negative,positive):\n",
        "    for i in range(len(d)):\n",
        "        words=d[i].split()\n",
        "        for j in range(len(words)):\n",
        "            for k in range(len(positive)):\n",
        "                if(words[j]==positive[k]):\n",
        "                    lab[i]=1\n",
        "                \n",
        "            for l in range(len(negative)): \n",
        "                if(words[j]==negative[l]):\n",
        "                    lab[i]=-1\n",
        "                \n",
        "           \n",
        "label(d,negative,positive)    \n",
        "print(lab)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z4xQeq4FOb8y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('label.txt', 'w') as f:\n",
        "    for item in lab:\n",
        "        f.write(\"%s\\n\" % item)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "453cYzYjOb9C",
        "colab_type": "code",
        "outputId": "dc70ec5d-7161-4ac7-cde5-300486008f94",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(lab)\n",
        "len(d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56691"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "GyUErRd8Ob9S",
        "colab_type": "code",
        "outputId": "6fae7aa8-da95-4957-a789-b0c746b6776c",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "filename = 'stopwords.txt'\n",
        "file = open(filename,\"r\" )\n",
        "stopwords = file.read().splitlines()\n",
        "file.close() \n",
        "\n",
        "words1=[]\n",
        "words =[]\n",
        "for i in range(0,len(d)):\n",
        "    words1=d[i].split()\n",
        "    for j in range(0,len(words1)):   \n",
        "        if(len(words1[j])<9):         #part of filtering\n",
        "            words.append(words1[j])\n",
        "        \n",
        "wordsf=[]\n",
        "words = [word.lower() for word in words]\n",
        "words = [word for word in words if word.isalpha()]\n",
        "wordsf=(set(words).difference(stopwords)) \n",
        "print(len(wordsf))\n",
        "words=list(wordsf)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "baGOZJQKOb9h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "count=[]\n",
        "for i in range(len(words)):\n",
        "    count.append(0)\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        if(words[i]==words[j]):\n",
        "            count[i]+=1\n",
        "            \n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MgQzaX73Ob9y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}